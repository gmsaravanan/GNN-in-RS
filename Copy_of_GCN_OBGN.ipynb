{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmsaravanan/GNN-in-RS/blob/main/Copy_of_GCN_OBGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OGBN-Products"
      ],
      "metadata": {
        "id": "g_Vh9kosaChl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ogbn-products dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. Node features are generated by extracting bag-of-words features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n",
        "\n",
        "The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels."
      ],
      "metadata": {
        "id": "8HFHJC-LaLyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkP8pA1qBE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008b94bb-9436-4510-8d70-8f6e820d8070"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hfxJ-qRg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e36f1d-fe23-4623-eeae-bc277edf470c"
      },
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=7427b20dd71205081d28bc7e5b94f8450f3811bd75a3b04f42803c57b1af0b89\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import DataLoader\n",
        "import numpy as np\n",
        "from torch_geometric.typing import SparseTensor\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3SkS1Mzcbe8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ibJ0ieoIwQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6575ed6-be31-4f81-f892-e22cbdd9897c"
      },
      "source": [
        "dataset_name = 'ogbn-products'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "# data = data.to(device)\n",
        "# split_idx = dataset.get_idx_split()\n",
        "# train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:18<00:00, 75.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 366.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check task type\n",
        "print('Task type: {}'.format(dataset.task_type))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp8pnCy5-j7l",
        "outputId": "5513e801-fc22-4356-d4ea-7c7eff210c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task type: multiclass classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ6I9prjdg57",
        "outputId": "cc28453e-c5e6-4c7b-90d1-b4f04b63d184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=2449029, x=[2449029, 100], y=[2449029, 1], adj_t=[2449029, 2449029, nnz=123718280])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.y.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-AYG-jcLyIl",
        "outputId": "ef47dc15-7a12-434e-e0e0-04260155437f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
              "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to have edge indxes to make a subgraph. We can get those from the adjacency matrix.\n",
        "data.edge_index = torch.stack([data.adj_t.__dict__[\"storage\"]._row, data.adj_t.__dict__[\"storage\"]._col])\n",
        "\n",
        "# We will only use the first 100000 nodes.\n",
        "sub_nodes = 100000\n",
        "sub_graph = data.subgraph(torch.arange(sub_nodes))\n",
        "\n",
        "# Update the adjaceny matrix according to the new graph\n",
        "sub_graph.adj_t = SparseTensor(\n",
        "    row=sub_graph.edge_index[0],\n",
        "    col=sub_graph.edge_index[1],\n",
        "    sparse_sizes=None,\n",
        "    is_sorted=True,\n",
        "    trust_data=True,\n",
        ")\n",
        "\n",
        "sub_graph = sub_graph.to(device)\n",
        "\n",
        "sub_graph\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN80UY6bALzs",
        "outputId": "31c03315-14d6-4dc9-dc73-b1e62aab1ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=100000, x=[100000, 100], y=[100000, 1], adj_t=[100000, 100000, nnz=2818046], edge_index=[2, 2818046])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt data into train validation and test set\n",
        "split_sizes = [int(sub_nodes*0.8),int(sub_nodes*0.05),int(sub_nodes*0.15)]\n",
        "indices = torch.arange(sub_nodes)\n",
        "np.random.shuffle(indices.numpy())\n",
        "split_idx = {s:t for t,s in zip(torch.split(indices, split_sizes, dim=0), [\"train\", \"valid\", \"test\"])}\n",
        "split_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6_6YFEBApJq",
        "outputId": "23475edb-a3f1-432b-a1a0-f88dc9c7d8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor([57061, 23004, 90477,  ..., 63101,  1775, 54225]),\n",
              " 'valid': tensor([97329, 40140,  9169,  ..., 32506, 42281, 73852]),\n",
              " 'test': tensor([82090, 95331, 67400,  ..., 56109, 79382, 88828])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size for data loaders\n",
        "batch_size = 32  # You can change this to your preferred batch size\n",
        "\n",
        "# Split indices for train, validation, and test sets\n",
        "train_idx = split_idx[\"train\"]\n",
        "valid_idx = split_idx[\"valid\"]\n",
        "test_idx = split_idx[\"test\"]"
      ],
      "metadata": {
        "id": "q3Nrhnno6ylE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 30,\n",
        "}\n",
        "args"
      ],
      "metadata": {
        "id": "bKuGGzv3_NOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435d3faf-ce02-49c0-ff5a-6ad49ac09b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'num_layers': 5,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.001,\n",
              " 'epochs': 30}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = [\n",
        "            GCNConv(input_dim, hidden_dim),\n",
        "        ]\n",
        "\n",
        "        for _ in range(num_layers-1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.last_layer = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = []\n",
        "        for _ in range(num_layers):\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.convs = torch.nn.ModuleList(self.convs)\n",
        "        self.bns = torch.nn.ModuleList(self.bns)\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "\n",
        "        # Probability of an element getting zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "        for i in range(len(self.convs)):\n",
        "            x = self.convs[i](x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.last_layer(x, adj_t)\n",
        "\n",
        "        if not self.return_embeds:\n",
        "          x = self.softmax(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "318M7d48P3JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(data.x, data.adj_t)  # Perform a single forward pass.\n",
        "    loss = loss_fn(out[train_idx], sub_graph.y[train_idx].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "kv8FysMPTpy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "cTjUhE2wT4sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 200,\n",
        "}\n",
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPSbM-YcT_Ik",
        "outputId": "fc04f3e1-061f-4670-f399-8c545d25d28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'num_layers': 3,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.01,\n",
              " 'epochs': 200}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(sub_graph.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "metadata": {
        "id": "UUhV70xiUhKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, sub_graph, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, sub_graph, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQblrAdGUkE9",
        "outputId": "afbdce4b-a5c8-4214-a0a4-3277b4811327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.0365, Train: 60.73%, Valid: 60.90% Test: 60.81%\n",
            "Epoch: 02, Loss: 1.6823, Train: 68.57%, Valid: 67.72% Test: 68.77%\n",
            "Epoch: 03, Loss: 1.0549, Train: 79.41%, Valid: 79.98% Test: 79.58%\n",
            "Epoch: 04, Loss: 0.8778, Train: 82.32%, Valid: 82.46% Test: 82.21%\n",
            "Epoch: 05, Loss: 0.7934, Train: 82.79%, Valid: 82.96% Test: 82.77%\n",
            "Epoch: 06, Loss: 0.7510, Train: 83.43%, Valid: 83.16% Test: 83.19%\n",
            "Epoch: 07, Loss: 0.7120, Train: 83.89%, Valid: 83.72% Test: 83.66%\n",
            "Epoch: 08, Loss: 0.6742, Train: 84.41%, Valid: 84.20% Test: 84.26%\n",
            "Epoch: 09, Loss: 0.6404, Train: 84.86%, Valid: 84.66% Test: 84.67%\n",
            "Epoch: 10, Loss: 0.6138, Train: 85.38%, Valid: 85.28% Test: 85.13%\n",
            "Epoch: 11, Loss: 0.5981, Train: 85.91%, Valid: 85.90% Test: 85.71%\n",
            "Epoch: 12, Loss: 0.5823, Train: 86.35%, Valid: 86.24% Test: 86.07%\n",
            "Epoch: 13, Loss: 0.5719, Train: 86.62%, Valid: 86.48% Test: 86.25%\n",
            "Epoch: 14, Loss: 0.5625, Train: 86.79%, Valid: 86.86% Test: 86.41%\n",
            "Epoch: 15, Loss: 0.5499, Train: 87.13%, Valid: 87.12% Test: 86.77%\n",
            "Epoch: 16, Loss: 0.5410, Train: 87.28%, Valid: 87.06% Test: 86.94%\n",
            "Epoch: 17, Loss: 0.5323, Train: 87.39%, Valid: 87.10% Test: 87.07%\n",
            "Epoch: 18, Loss: 0.5255, Train: 87.48%, Valid: 87.12% Test: 87.19%\n",
            "Epoch: 19, Loss: 0.5205, Train: 87.57%, Valid: 87.20% Test: 87.26%\n",
            "Epoch: 20, Loss: 0.5125, Train: 87.72%, Valid: 87.30% Test: 87.31%\n",
            "Epoch: 21, Loss: 0.5082, Train: 87.80%, Valid: 87.56% Test: 87.48%\n",
            "Epoch: 22, Loss: 0.4991, Train: 87.89%, Valid: 87.48% Test: 87.59%\n",
            "Epoch: 23, Loss: 0.4946, Train: 87.99%, Valid: 87.64% Test: 87.65%\n",
            "Epoch: 24, Loss: 0.4905, Train: 87.94%, Valid: 87.62% Test: 87.67%\n",
            "Epoch: 25, Loss: 0.4831, Train: 87.98%, Valid: 87.64% Test: 87.69%\n",
            "Epoch: 26, Loss: 0.4791, Train: 88.07%, Valid: 87.82% Test: 87.73%\n",
            "Epoch: 27, Loss: 0.4755, Train: 88.17%, Valid: 87.88% Test: 87.78%\n",
            "Epoch: 28, Loss: 0.4700, Train: 88.25%, Valid: 88.06% Test: 87.88%\n",
            "Epoch: 29, Loss: 0.4638, Train: 88.38%, Valid: 88.14% Test: 87.89%\n",
            "Epoch: 30, Loss: 0.4589, Train: 88.54%, Valid: 88.20% Test: 88.08%\n",
            "Epoch: 31, Loss: 0.4588, Train: 88.63%, Valid: 88.26% Test: 88.13%\n",
            "Epoch: 32, Loss: 0.4540, Train: 88.69%, Valid: 88.34% Test: 88.09%\n",
            "Epoch: 33, Loss: 0.4499, Train: 88.69%, Valid: 88.56% Test: 88.06%\n",
            "Epoch: 34, Loss: 0.4481, Train: 88.72%, Valid: 88.54% Test: 88.14%\n",
            "Epoch: 35, Loss: 0.4449, Train: 88.79%, Valid: 88.58% Test: 88.21%\n",
            "Epoch: 36, Loss: 0.4407, Train: 88.80%, Valid: 88.48% Test: 88.23%\n",
            "Epoch: 37, Loss: 0.4381, Train: 88.88%, Valid: 88.58% Test: 88.22%\n",
            "Epoch: 38, Loss: 0.4362, Train: 88.95%, Valid: 88.56% Test: 88.35%\n",
            "Epoch: 39, Loss: 0.4338, Train: 89.03%, Valid: 88.66% Test: 88.39%\n",
            "Epoch: 40, Loss: 0.4294, Train: 89.09%, Valid: 88.74% Test: 88.48%\n",
            "Epoch: 41, Loss: 0.4276, Train: 89.14%, Valid: 88.92% Test: 88.59%\n",
            "Epoch: 42, Loss: 0.4255, Train: 89.25%, Valid: 89.04% Test: 88.68%\n",
            "Epoch: 43, Loss: 0.4206, Train: 89.29%, Valid: 88.88% Test: 88.70%\n",
            "Epoch: 44, Loss: 0.4196, Train: 89.30%, Valid: 88.94% Test: 88.79%\n",
            "Epoch: 45, Loss: 0.4187, Train: 89.31%, Valid: 89.06% Test: 88.76%\n",
            "Epoch: 46, Loss: 0.4172, Train: 89.36%, Valid: 89.06% Test: 88.81%\n",
            "Epoch: 47, Loss: 0.4155, Train: 89.38%, Valid: 89.06% Test: 88.83%\n",
            "Epoch: 48, Loss: 0.4105, Train: 89.42%, Valid: 89.16% Test: 88.79%\n",
            "Epoch: 49, Loss: 0.4105, Train: 89.55%, Valid: 89.22% Test: 88.94%\n",
            "Epoch: 50, Loss: 0.4085, Train: 89.59%, Valid: 89.24% Test: 89.00%\n",
            "Epoch: 51, Loss: 0.4058, Train: 89.65%, Valid: 89.38% Test: 89.12%\n",
            "Epoch: 52, Loss: 0.4043, Train: 89.71%, Valid: 89.48% Test: 89.14%\n",
            "Epoch: 53, Loss: 0.4011, Train: 89.74%, Valid: 89.44% Test: 89.15%\n",
            "Epoch: 54, Loss: 0.4005, Train: 89.77%, Valid: 89.42% Test: 89.10%\n",
            "Epoch: 55, Loss: 0.3993, Train: 89.82%, Valid: 89.40% Test: 89.18%\n",
            "Epoch: 56, Loss: 0.3968, Train: 89.86%, Valid: 89.38% Test: 89.21%\n",
            "Epoch: 57, Loss: 0.3951, Train: 89.89%, Valid: 89.56% Test: 89.28%\n",
            "Epoch: 58, Loss: 0.3936, Train: 89.87%, Valid: 89.60% Test: 89.29%\n",
            "Epoch: 59, Loss: 0.3926, Train: 89.91%, Valid: 89.56% Test: 89.27%\n",
            "Epoch: 60, Loss: 0.3918, Train: 90.00%, Valid: 89.68% Test: 89.29%\n",
            "Epoch: 61, Loss: 0.3901, Train: 90.02%, Valid: 89.66% Test: 89.43%\n",
            "Epoch: 62, Loss: 0.3874, Train: 90.01%, Valid: 89.58% Test: 89.41%\n",
            "Epoch: 63, Loss: 0.3867, Train: 90.08%, Valid: 89.60% Test: 89.46%\n",
            "Epoch: 64, Loss: 0.3836, Train: 90.09%, Valid: 89.84% Test: 89.35%\n",
            "Epoch: 65, Loss: 0.3847, Train: 90.16%, Valid: 89.92% Test: 89.52%\n",
            "Epoch: 66, Loss: 0.3826, Train: 90.14%, Valid: 89.96% Test: 89.43%\n",
            "Epoch: 67, Loss: 0.3812, Train: 90.21%, Valid: 89.88% Test: 89.53%\n",
            "Epoch: 68, Loss: 0.3776, Train: 90.22%, Valid: 89.86% Test: 89.51%\n",
            "Epoch: 69, Loss: 0.3782, Train: 90.25%, Valid: 89.92% Test: 89.57%\n",
            "Epoch: 70, Loss: 0.3760, Train: 90.28%, Valid: 89.94% Test: 89.51%\n",
            "Epoch: 71, Loss: 0.3760, Train: 90.30%, Valid: 89.96% Test: 89.43%\n",
            "Epoch: 72, Loss: 0.3753, Train: 90.38%, Valid: 89.98% Test: 89.56%\n",
            "Epoch: 73, Loss: 0.3723, Train: 90.44%, Valid: 90.08% Test: 89.58%\n",
            "Epoch: 74, Loss: 0.3728, Train: 90.41%, Valid: 90.18% Test: 89.77%\n",
            "Epoch: 75, Loss: 0.3713, Train: 90.40%, Valid: 90.16% Test: 89.62%\n",
            "Epoch: 76, Loss: 0.3711, Train: 90.40%, Valid: 90.22% Test: 89.60%\n",
            "Epoch: 77, Loss: 0.3686, Train: 90.46%, Valid: 90.18% Test: 89.61%\n",
            "Epoch: 78, Loss: 0.3674, Train: 90.50%, Valid: 90.16% Test: 89.65%\n",
            "Epoch: 79, Loss: 0.3660, Train: 90.54%, Valid: 90.24% Test: 89.63%\n",
            "Epoch: 80, Loss: 0.3635, Train: 90.51%, Valid: 90.12% Test: 89.71%\n",
            "Epoch: 81, Loss: 0.3634, Train: 90.53%, Valid: 90.26% Test: 89.73%\n",
            "Epoch: 82, Loss: 0.3644, Train: 90.57%, Valid: 90.32% Test: 89.74%\n",
            "Epoch: 83, Loss: 0.3629, Train: 90.61%, Valid: 90.36% Test: 89.75%\n",
            "Epoch: 84, Loss: 0.3618, Train: 90.60%, Valid: 90.34% Test: 89.77%\n",
            "Epoch: 85, Loss: 0.3613, Train: 90.56%, Valid: 90.38% Test: 89.82%\n",
            "Epoch: 86, Loss: 0.3590, Train: 90.61%, Valid: 90.30% Test: 89.85%\n",
            "Epoch: 87, Loss: 0.3578, Train: 90.61%, Valid: 90.28% Test: 89.73%\n",
            "Epoch: 88, Loss: 0.3579, Train: 90.75%, Valid: 90.32% Test: 89.71%\n",
            "Epoch: 89, Loss: 0.3558, Train: 90.77%, Valid: 90.28% Test: 89.76%\n",
            "Epoch: 90, Loss: 0.3540, Train: 90.71%, Valid: 90.38% Test: 89.80%\n",
            "Epoch: 91, Loss: 0.3552, Train: 90.70%, Valid: 90.26% Test: 89.89%\n",
            "Epoch: 92, Loss: 0.3550, Train: 90.83%, Valid: 90.22% Test: 89.85%\n",
            "Epoch: 93, Loss: 0.3548, Train: 90.83%, Valid: 90.26% Test: 89.80%\n",
            "Epoch: 94, Loss: 0.3513, Train: 90.82%, Valid: 90.18% Test: 89.87%\n",
            "Epoch: 95, Loss: 0.3516, Train: 90.79%, Valid: 90.34% Test: 89.98%\n",
            "Epoch: 96, Loss: 0.3505, Train: 90.79%, Valid: 90.12% Test: 89.91%\n",
            "Epoch: 97, Loss: 0.3484, Train: 90.85%, Valid: 90.26% Test: 90.02%\n",
            "Epoch: 98, Loss: 0.3490, Train: 90.92%, Valid: 90.46% Test: 89.92%\n",
            "Epoch: 99, Loss: 0.3470, Train: 90.98%, Valid: 90.66% Test: 89.94%\n",
            "Epoch: 100, Loss: 0.3453, Train: 90.94%, Valid: 90.86% Test: 90.02%\n",
            "Epoch: 101, Loss: 0.3468, Train: 90.96%, Valid: 90.64% Test: 90.03%\n",
            "Epoch: 102, Loss: 0.3452, Train: 91.02%, Valid: 90.64% Test: 90.16%\n",
            "Epoch: 103, Loss: 0.3439, Train: 90.95%, Valid: 90.54% Test: 90.09%\n",
            "Epoch: 104, Loss: 0.3432, Train: 91.00%, Valid: 90.62% Test: 90.05%\n",
            "Epoch: 105, Loss: 0.3435, Train: 90.96%, Valid: 90.68% Test: 90.16%\n",
            "Epoch: 106, Loss: 0.3429, Train: 91.05%, Valid: 90.84% Test: 90.08%\n",
            "Epoch: 107, Loss: 0.3396, Train: 90.92%, Valid: 90.64% Test: 89.98%\n",
            "Epoch: 108, Loss: 0.3411, Train: 90.87%, Valid: 90.50% Test: 90.11%\n",
            "Epoch: 109, Loss: 0.3398, Train: 91.01%, Valid: 90.76% Test: 90.26%\n",
            "Epoch: 110, Loss: 0.3401, Train: 91.09%, Valid: 90.46% Test: 90.25%\n",
            "Epoch: 111, Loss: 0.3399, Train: 90.97%, Valid: 90.38% Test: 89.96%\n",
            "Epoch: 112, Loss: 0.3404, Train: 91.04%, Valid: 90.60% Test: 89.98%\n",
            "Epoch: 113, Loss: 0.3390, Train: 91.14%, Valid: 90.76% Test: 90.23%\n",
            "Epoch: 114, Loss: 0.3358, Train: 91.04%, Valid: 90.76% Test: 90.11%\n",
            "Epoch: 115, Loss: 0.3367, Train: 91.16%, Valid: 90.54% Test: 90.22%\n",
            "Epoch: 116, Loss: 0.3342, Train: 91.14%, Valid: 90.58% Test: 90.17%\n",
            "Epoch: 117, Loss: 0.3352, Train: 91.16%, Valid: 90.62% Test: 90.27%\n",
            "Epoch: 118, Loss: 0.3344, Train: 91.20%, Valid: 90.84% Test: 90.08%\n",
            "Epoch: 119, Loss: 0.3343, Train: 91.23%, Valid: 90.58% Test: 90.17%\n",
            "Epoch: 120, Loss: 0.3339, Train: 91.20%, Valid: 90.70% Test: 90.18%\n",
            "Epoch: 121, Loss: 0.3310, Train: 91.22%, Valid: 90.74% Test: 90.17%\n",
            "Epoch: 122, Loss: 0.3330, Train: 91.27%, Valid: 90.84% Test: 90.25%\n",
            "Epoch: 123, Loss: 0.3302, Train: 91.23%, Valid: 90.74% Test: 90.04%\n",
            "Epoch: 124, Loss: 0.3307, Train: 91.34%, Valid: 90.72% Test: 90.24%\n",
            "Epoch: 125, Loss: 0.3298, Train: 91.36%, Valid: 90.80% Test: 90.32%\n",
            "Epoch: 126, Loss: 0.3277, Train: 91.36%, Valid: 90.78% Test: 90.44%\n",
            "Epoch: 127, Loss: 0.3260, Train: 91.36%, Valid: 90.86% Test: 90.31%\n",
            "Epoch: 128, Loss: 0.3271, Train: 91.33%, Valid: 90.80% Test: 90.33%\n",
            "Epoch: 129, Loss: 0.3270, Train: 91.31%, Valid: 90.82% Test: 90.21%\n",
            "Epoch: 130, Loss: 0.3263, Train: 91.38%, Valid: 90.82% Test: 90.32%\n",
            "Epoch: 131, Loss: 0.3244, Train: 91.50%, Valid: 90.88% Test: 90.29%\n",
            "Epoch: 132, Loss: 0.3249, Train: 91.50%, Valid: 90.92% Test: 90.29%\n",
            "Epoch: 133, Loss: 0.3246, Train: 91.47%, Valid: 90.78% Test: 90.30%\n",
            "Epoch: 134, Loss: 0.3216, Train: 91.53%, Valid: 90.70% Test: 90.31%\n",
            "Epoch: 135, Loss: 0.3217, Train: 91.52%, Valid: 90.70% Test: 90.40%\n",
            "Epoch: 136, Loss: 0.3199, Train: 91.47%, Valid: 90.74% Test: 90.35%\n",
            "Epoch: 137, Loss: 0.3231, Train: 91.50%, Valid: 90.82% Test: 90.14%\n",
            "Epoch: 138, Loss: 0.3221, Train: 91.47%, Valid: 90.86% Test: 90.33%\n",
            "Epoch: 139, Loss: 0.3211, Train: 91.55%, Valid: 90.84% Test: 90.35%\n",
            "Epoch: 140, Loss: 0.3200, Train: 91.36%, Valid: 90.68% Test: 90.23%\n",
            "Epoch: 141, Loss: 0.3221, Train: 91.47%, Valid: 91.02% Test: 90.32%\n",
            "Epoch: 142, Loss: 0.3208, Train: 91.56%, Valid: 90.86% Test: 90.39%\n",
            "Epoch: 143, Loss: 0.3200, Train: 91.55%, Valid: 90.80% Test: 90.33%\n",
            "Epoch: 144, Loss: 0.3185, Train: 91.58%, Valid: 90.84% Test: 90.32%\n",
            "Epoch: 145, Loss: 0.3162, Train: 91.50%, Valid: 90.78% Test: 90.18%\n",
            "Epoch: 146, Loss: 0.3200, Train: 91.53%, Valid: 90.66% Test: 90.45%\n",
            "Epoch: 147, Loss: 0.3163, Train: 91.63%, Valid: 90.64% Test: 90.50%\n",
            "Epoch: 148, Loss: 0.3171, Train: 91.47%, Valid: 90.82% Test: 90.33%\n",
            "Epoch: 149, Loss: 0.3187, Train: 91.67%, Valid: 91.08% Test: 90.53%\n",
            "Epoch: 150, Loss: 0.3135, Train: 91.60%, Valid: 90.88% Test: 90.40%\n",
            "Epoch: 151, Loss: 0.3150, Train: 91.50%, Valid: 90.74% Test: 90.29%\n",
            "Epoch: 152, Loss: 0.3162, Train: 91.64%, Valid: 90.86% Test: 90.56%\n",
            "Epoch: 153, Loss: 0.3128, Train: 91.61%, Valid: 90.90% Test: 90.41%\n",
            "Epoch: 154, Loss: 0.3142, Train: 91.78%, Valid: 91.12% Test: 90.41%\n",
            "Epoch: 155, Loss: 0.3135, Train: 91.78%, Valid: 90.94% Test: 90.52%\n",
            "Epoch: 156, Loss: 0.3097, Train: 91.75%, Valid: 90.94% Test: 90.51%\n",
            "Epoch: 157, Loss: 0.3108, Train: 91.75%, Valid: 90.96% Test: 90.54%\n",
            "Epoch: 158, Loss: 0.3120, Train: 91.74%, Valid: 91.00% Test: 90.34%\n",
            "Epoch: 159, Loss: 0.3101, Train: 91.67%, Valid: 90.86% Test: 90.48%\n",
            "Epoch: 160, Loss: 0.3108, Train: 91.77%, Valid: 90.86% Test: 90.60%\n",
            "Epoch: 161, Loss: 0.3097, Train: 91.77%, Valid: 90.82% Test: 90.49%\n",
            "Epoch: 162, Loss: 0.3091, Train: 91.77%, Valid: 90.78% Test: 90.25%\n",
            "Epoch: 163, Loss: 0.3088, Train: 91.75%, Valid: 90.74% Test: 90.33%\n",
            "Epoch: 164, Loss: 0.3070, Train: 91.70%, Valid: 90.88% Test: 90.33%\n",
            "Epoch: 165, Loss: 0.3077, Train: 91.80%, Valid: 91.16% Test: 90.40%\n",
            "Epoch: 166, Loss: 0.3094, Train: 91.83%, Valid: 90.96% Test: 90.41%\n",
            "Epoch: 167, Loss: 0.3065, Train: 91.79%, Valid: 90.78% Test: 90.44%\n",
            "Epoch: 168, Loss: 0.3047, Train: 91.76%, Valid: 91.08% Test: 90.51%\n",
            "Epoch: 169, Loss: 0.3100, Train: 91.92%, Valid: 91.14% Test: 90.50%\n",
            "Epoch: 170, Loss: 0.3055, Train: 91.90%, Valid: 91.16% Test: 90.52%\n",
            "Epoch: 171, Loss: 0.3049, Train: 91.67%, Valid: 91.10% Test: 90.47%\n",
            "Epoch: 172, Loss: 0.3055, Train: 91.78%, Valid: 91.06% Test: 90.61%\n",
            "Epoch: 173, Loss: 0.3063, Train: 91.83%, Valid: 90.84% Test: 90.55%\n",
            "Epoch: 174, Loss: 0.3039, Train: 91.72%, Valid: 90.86% Test: 90.41%\n",
            "Epoch: 175, Loss: 0.3055, Train: 91.92%, Valid: 90.94% Test: 90.59%\n",
            "Epoch: 176, Loss: 0.3013, Train: 91.94%, Valid: 91.20% Test: 90.73%\n",
            "Epoch: 177, Loss: 0.3048, Train: 91.62%, Valid: 90.74% Test: 90.38%\n",
            "Epoch: 178, Loss: 0.3025, Train: 91.95%, Valid: 91.12% Test: 90.53%\n",
            "Epoch: 179, Loss: 0.2995, Train: 91.90%, Valid: 91.12% Test: 90.57%\n",
            "Epoch: 180, Loss: 0.3027, Train: 91.93%, Valid: 91.06% Test: 90.50%\n",
            "Epoch: 181, Loss: 0.3015, Train: 91.99%, Valid: 91.22% Test: 90.55%\n",
            "Epoch: 182, Loss: 0.3004, Train: 91.88%, Valid: 90.98% Test: 90.47%\n",
            "Epoch: 183, Loss: 0.3042, Train: 91.98%, Valid: 91.28% Test: 90.54%\n",
            "Epoch: 184, Loss: 0.3004, Train: 92.03%, Valid: 91.16% Test: 90.64%\n",
            "Epoch: 185, Loss: 0.2990, Train: 91.99%, Valid: 91.06% Test: 90.55%\n",
            "Epoch: 186, Loss: 0.2997, Train: 91.95%, Valid: 91.12% Test: 90.67%\n",
            "Epoch: 187, Loss: 0.3001, Train: 91.92%, Valid: 90.88% Test: 90.76%\n",
            "Epoch: 188, Loss: 0.2967, Train: 91.93%, Valid: 90.92% Test: 90.56%\n",
            "Epoch: 189, Loss: 0.2972, Train: 92.08%, Valid: 91.16% Test: 90.65%\n",
            "Epoch: 190, Loss: 0.2964, Train: 92.00%, Valid: 90.94% Test: 90.55%\n",
            "Epoch: 191, Loss: 0.2973, Train: 91.92%, Valid: 90.84% Test: 90.39%\n",
            "Epoch: 192, Loss: 0.2978, Train: 91.83%, Valid: 90.86% Test: 90.45%\n",
            "Epoch: 193, Loss: 0.2974, Train: 91.86%, Valid: 90.86% Test: 90.44%\n",
            "Epoch: 194, Loss: 0.2987, Train: 91.93%, Valid: 90.90% Test: 90.45%\n",
            "Epoch: 195, Loss: 0.2972, Train: 92.06%, Valid: 91.22% Test: 90.65%\n",
            "Epoch: 196, Loss: 0.2930, Train: 92.04%, Valid: 91.10% Test: 90.76%\n",
            "Epoch: 197, Loss: 0.2970, Train: 92.07%, Valid: 91.18% Test: 90.72%\n",
            "Epoch: 198, Loss: 0.2940, Train: 92.11%, Valid: 91.08% Test: 90.59%\n",
            "Epoch: 199, Loss: 0.2937, Train: 92.13%, Valid: 91.04% Test: 90.69%\n",
            "Epoch: 200, Loss: 0.2958, Train: 92.09%, Valid: 91.20% Test: 90.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_result = test(best_model, sub_graph, split_idx, evaluator, save_model_results=True)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KcoDqzpUnFl",
        "outputId": "77325fb4-4543-4b0d-945a-80f8d6f4efc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 91.98%, Valid: 91.28% Test: 90.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78PqBwzrkWh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}